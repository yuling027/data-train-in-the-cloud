============================= test session starts ==============================
platform darwin -- Python 3.10.6, pytest-8.4.1, pluggy-1.6.0 -- /Users/yulingsun/.pyenv/versions/3.10.6/envs/taxifare-env/bin/python
cachedir: .pytest_cache
rootdir: /Users/yulingsun/code/yuling027/07-ML-Ops/02-Cloud-training/data-train-in-the-cloud/tests
configfile: pytest_kitt.ini
collecting ... collected 9 items

tests/cloud_training/test_cloud_data.py::TestCloudData::test_big_query_dataset_variable_exists PASSED [ 11%]
tests/cloud_training/test_cloud_data.py::TestCloudData::test_cloud_data_create_dataset PASSED [ 22%]
tests/cloud_training/test_cloud_data.py::TestCloudData::test_cloud_data_create_table PASSED [ 33%]
tests/cloud_training/test_main.py::TestMain::test_route_preprocess FAILED [ 44%]
tests/cloud_training/test_main.py::TestMain::test_route_train[local] FAILED [ 55%]
tests/cloud_training/test_main.py::TestMain::test_route_train[gcs] FAILED [ 66%]
tests/cloud_training/test_main.py::TestMain::test_route_evaluate FAILED  [ 77%]
tests/cloud_training/test_main.py::TestMain::test_route_pred FAILED      [ 88%]
tests/cloud_training/test_vm.py::test_i_am_a_vm FAILED                   [100%]

=================================== FAILURES ===================================
________________________ TestMain.test_route_preprocess ________________________

self = <tests.cloud_training.test_main.TestMain object at 0x11992d390>
fixture_query_1k =      fare_amount           pickup_datetime  ...  dropoff_latitude  passenger_count
0            8.9 2009-01-15 09:22:3...           4
454          8.5 2014-12-27 16:47:42+00:00  ...         40.771263                4

[455 rows x 7 columns]

    def test_route_preprocess(self, fixture_query_1k):
    
        from taxifare.interface.main import preprocess
    
        data_query_path = Path(LOCAL_DATA_PATH).joinpath("raw",f"query_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv")
        data_processed_path = Path(LOCAL_DATA_PATH).joinpath("processed",f"processed_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv")
    
        data_query_exists = data_query_path.is_file()
        data_processed_exists = data_processed_path.is_file()
    
        # SETUP
        if data_query_exists:
            shutil.copyfile(data_query_path, f'{data_query_path}_backup')
            data_query_path.unlink()
        if data_processed_exists:
            shutil.copyfile(data_processed_path, f'{data_processed_path}_backup')
            data_processed_path.unlink()
    
        # ACT
        # TODO: add try-except to be certain of reseting state if crash
        # Check route runs querying Big Query
        preprocess(min_date=MIN_DATE, max_date=MAX_DATE)
    
        # Load newly saved query data and test it against true fixture
>       data_query = pd.read_csv(data_query_path, parse_dates=["pickup_datetime"])

tests/cloud_training/test_main.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/util/_decorators.py:211: in wrapper
    return func(*args, **kwargs)
../../../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/util/_decorators.py:331: in wrapper
    return func(*args, **kwargs)
../../../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950: in read_csv
    return _read(filepath_or_buffer, kwds)
../../../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605: in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
../../../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442: in __init__
    self._engine = self._make_engine(f, self.engine)
../../../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735: in _make_engine
    self.handles = get_handle(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

path_or_buf = PosixPath('/Users/yulingsun/.lewagon/mlops/data/raw/query_2009-01-01_2015-01-01_1k.csv')
mode = 'r'

    @doc(compression_options=_shared_docs["compression_options"] % "path_or_buf")
    def get_handle(
        path_or_buf: FilePath | BaseBuffer,
        mode: str,
        *,
        encoding: str | None = None,
        compression: CompressionOptions = None,
        memory_map: bool = False,
        is_text: bool = True,
        errors: str | None = None,
        storage_options: StorageOptions = None,
    ) -> IOHandles[str] | IOHandles[bytes]:
        """
        Get file handle for given path/buffer and mode.
    
        Parameters
        ----------
        path_or_buf : str or file handle
            File path or object.
        mode : str
            Mode to open path_or_buf with.
        encoding : str or None
            Encoding to use.
        {compression_options}
    
            .. versionchanged:: 1.0.0
               May now be a dict with key 'method' as compression mode
               and other keys as compression options if compression
               mode is 'zip'.
    
            .. versionchanged:: 1.1.0
               Passing compression options as keys in dict is now
               supported for compression modes 'gzip', 'bz2', 'zstd' and 'zip'.
    
            .. versionchanged:: 1.4.0 Zstandard support.
    
        memory_map : bool, default False
            See parsers._parser_params for more information. Only used by read_csv.
        is_text : bool, default True
            Whether the type of the content passed to the file/buffer is string or
            bytes. This is not the same as `"b" not in mode`. If a string content is
            passed to a binary file/buffer, a wrapper is inserted.
        errors : str, default 'strict'
            Specifies how encoding and decoding errors are to be handled.
            See the errors argument for :func:`open` for a full list
            of options.
        storage_options: StorageOptions = None
            Passed to _get_filepath_or_buffer
    
        .. versionchanged:: 1.2.0
    
        Returns the dataclass IOHandles
        """
        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior
        encoding = encoding or "utf-8"
    
        errors = errors or "strict"
    
        # read_csv does not know whether the buffer is opened in binary/text mode
        if _is_binary_mode(path_or_buf, mode) and "b" not in mode:
            mode += "b"
    
        # validate encoding and errors
        codecs.lookup(encoding)
        if isinstance(errors, str):
            codecs.lookup_error(errors)
    
        # open URLs
        ioargs = _get_filepath_or_buffer(
            path_or_buf,
            encoding=encoding,
            compression=compression,
            mode=mode,
            storage_options=storage_options,
        )
    
        handle = ioargs.filepath_or_buffer
        handles: list[BaseBuffer]
    
        # memory mapping needs to be the first step
        # only used for read_csv
        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)
    
        is_path = isinstance(handle, str)
        compression_args = dict(ioargs.compression)
        compression = compression_args.pop("method")
    
        # Only for write methods
        if "r" not in mode and is_path:
            check_parent_directory(str(handle))
    
        if compression:
            if compression != "zstd":
                # compression libraries do not like an explicit text-mode
                ioargs.mode = ioargs.mode.replace("t", "")
            elif compression == "zstd" and "b" not in ioargs.mode:
                # python-zstandard defaults to text mode, but we always expect
                # compression libraries to use binary mode.
                ioargs.mode += "b"
    
            # GZ Compression
            if compression == "gzip":
                if isinstance(handle, str):
                    # error: Incompatible types in assignment (expression has type
                    # "GzipFile", variable has type "Union[str, BaseBuffer]")
                    handle = gzip.GzipFile(  # type: ignore[assignment]
                        filename=handle,
                        mode=ioargs.mode,
                        **compression_args,
                    )
                else:
                    handle = gzip.GzipFile(
                        # No overload variant of "GzipFile" matches argument types
                        # "Union[str, BaseBuffer]", "str", "Dict[str, Any]"
                        fileobj=handle,  # type: ignore[call-overload]
                        mode=ioargs.mode,
                        **compression_args,
                    )
    
            # BZ Compression
            elif compression == "bz2":
                # No overload variant of "BZ2File" matches argument types
                # "Union[str, BaseBuffer]", "str", "Dict[str, Any]"
                handle = bz2.BZ2File(  # type: ignore[call-overload]
                    handle,
                    mode=ioargs.mode,
                    **compression_args,
                )
    
            # ZIP Compression
            elif compression == "zip":
                # error: Argument 1 to "_BytesZipFile" has incompatible type
                # "Union[str, BaseBuffer]"; expected "Union[Union[str, PathLike[str]],
                # ReadBuffer[bytes], WriteBuffer[bytes]]"
                handle = _BytesZipFile(
                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]
                )
                if handle.buffer.mode == "r":
                    handles.append(handle)
                    zip_names = handle.buffer.namelist()
                    if len(zip_names) == 1:
                        handle = handle.buffer.open(zip_names.pop())
                    elif not zip_names:
                        raise ValueError(f"Zero files found in ZIP file {path_or_buf}")
                    else:
                        raise ValueError(
                            "Multiple files found in ZIP file. "
                            f"Only one file per ZIP: {zip_names}"
                        )
    
            # TAR Encoding
            elif compression == "tar":
                compression_args.setdefault("mode", ioargs.mode)
                if isinstance(handle, str):
                    handle = _BytesTarFile(name=handle, **compression_args)
                else:
                    # error: Argument "fileobj" to "_BytesTarFile" has incompatible
                    # type "BaseBuffer"; expected "Union[ReadBuffer[bytes],
                    # WriteBuffer[bytes], None]"
                    handle = _BytesTarFile(
                        fileobj=handle, **compression_args  # type: ignore[arg-type]
                    )
                assert isinstance(handle, _BytesTarFile)
                if "r" in handle.buffer.mode:
                    handles.append(handle)
                    files = handle.buffer.getnames()
                    if len(files) == 1:
                        file = handle.buffer.extractfile(files[0])
                        assert file is not None
                        handle = file
                    elif not files:
                        raise ValueError(f"Zero files found in TAR archive {path_or_buf}")
                    else:
                        raise ValueError(
                            "Multiple files found in TAR archive. "
                            f"Only one file per TAR archive: {files}"
                        )
    
            # XZ Compression
            elif compression == "xz":
                # error: Argument 1 to "LZMAFile" has incompatible type "Union[str,
                # BaseBuffer]"; expected "Optional[Union[Union[str, bytes, PathLike[str],
                # PathLike[bytes]], IO[bytes]]]"
                handle = get_lzma_file()(handle, ioargs.mode)  # type: ignore[arg-type]
    
            # Zstd Compression
            elif compression == "zstd":
                zstd = import_optional_dependency("zstandard")
                if "r" in ioargs.mode:
                    open_args = {"dctx": zstd.ZstdDecompressor(**compression_args)}
                else:
                    open_args = {"cctx": zstd.ZstdCompressor(**compression_args)}
                handle = zstd.open(
                    handle,
                    mode=ioargs.mode,
                    **open_args,
                )
    
            # Unrecognized Compression
            else:
                msg = f"Unrecognized compression type: {compression}"
                raise ValueError(msg)
    
            assert not isinstance(handle, str)
            handles.append(handle)
    
        elif isinstance(handle, str):
            # Check whether the filename is to be opened in binary mode.
            # Binary mode does not support 'encoding' and 'newline'.
            if ioargs.encoding and "b" not in ioargs.mode:
                # Encoding
>               handle = open(
                    handle,
                    ioargs.mode,
                    encoding=ioargs.encoding,
                    errors=errors,
                    newline="",
                )
E               FileNotFoundError: [Errno 2] No such file or directory: '/Users/yulingsun/.lewagon/mlops/data/raw/query_2009-01-01_2015-01-01_1k.csv'

../../../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/common.py:856: FileNotFoundError
----------------------------- Captured stdout call -----------------------------
[34m
Loading TensorFlow...[0m

✅ TensorFlow loaded (8.82s)
[35m
 ⭐️ Use case: preprocess[0m
✅ preprocess() done 

----------------------------- Captured stderr call -----------------------------
2025-08-26 12:00:20.845802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
_______________________ TestMain.test_route_train[local] _______________________

self = <tests.cloud_training.test_main.TestMain object at 0x11992d600>
fixture_processed_1k =            0    1    2    3    4    5   ...   60   61   62   63   64         65
0    0.000000  0.0  0.0  0.0  1.0  0.0...0.0   6.500000
446  0.428571  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   8.500000

[447 rows x 66 columns]
model_target = 'local'

    @pytest.mark.parametrize('model_target', ['local' , 'gcs'])
    def test_route_train(self, fixture_processed_1k, model_target):
        """Test route train behave as expected, for various context of LOCAL or GCS model storage"""
    
        # 1) SETUP
        old_model_target = os.environ.get("MODEL_TARGET")
        os.environ.update(MODEL_TARGET=model_target)
    
        data_processed_path = Path(LOCAL_DATA_PATH).joinpath("processed",f"processed_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv")
        data_processed_exists = data_processed_path.is_file()
        if data_processed_exists:
            shutil.copyfile(data_processed_path, f'{data_processed_path}_backup')
            data_processed_path.unlink()
    
        data_processed_fixture_path = "https://storage.googleapis.com/datascience-mlops/taxi-fare-ny/solutions/data_processed_fixture_2009-01-01_2015-01-01_1k.csv"
        os.system(f"curl {data_processed_fixture_path} > {data_processed_path}")
    
        # 2) ACT
        from taxifare.interface.main import train
    
        # Train it from Big Query
>       train(min_date=MIN_DATE, max_date=MAX_DATE, learning_rate=0.01, patience=0)

tests/cloud_training/test_main.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

min_date = '2009-01-01', max_date = '2015-01-01', split_ratio = 0.02
learning_rate = 0.01, batch_size = 256, patience = 0

    def train(
            min_date:str = '2009-01-01',
            max_date:str = '2015-01-01',
            split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set
            learning_rate=0.0005,
            batch_size = 256,
            patience = 2
        ) -> float:
    
        """
        - Download processed data from your BQ table (or from cache if it exists)
        - Train on the preprocessed dataset (which should be ordered by date)
        - Store training results and model weights
    
        Return val_mae as a float
        """
    
        print(Fore.MAGENTA + "\n⭐️ Use case: train" + Style.RESET_ALL)
        print(Fore.BLUE + "\nLoading preprocessed validation data..." + Style.RESET_ALL)
    
        min_date = parse(min_date).strftime('%Y-%m-%d') # e.g '2009-01-01'
        max_date = parse(max_date).strftime('%Y-%m-%d') # e.g '2009-01-01'
    
        # Load processed data using `get_data_with_cache` in chronological order
        # Try it out manually on console.cloud.google.com first!
    
        pass  # YOUR CODE HERE
    
        # Create (X_train_processed, y_train, X_val_processed, y_val)
        pass  # YOUR CODE HERE
    
        # Train model using `model.py`
        pass  # YOUR CODE HERE
    
>       val_mae = np.min(history.history['val_mae'])
E       NameError: name 'history' is not defined

taxifare/interface/main.py:78: NameError
----------------------------- Captured stdout call -----------------------------
[35m
⭐️ Use case: train[0m
[34m
Loading preprocessed validation data...[0m
----------------------------- Captured stderr call -----------------------------
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  153k  100  153k    0     0  1352k      0 --:--:-- --:--:-- --:--:-- 1362k
________________________ TestMain.test_route_train[gcs] ________________________

self = <tests.cloud_training.test_main.TestMain object at 0x11992d690>
fixture_processed_1k =            0    1    2    3    4    5   ...   60   61   62   63   64         65
0    0.000000  0.0  0.0  0.0  1.0  0.0...0.0   6.500000
446  0.428571  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   8.500000

[447 rows x 66 columns]
model_target = 'gcs'

    @pytest.mark.parametrize('model_target', ['local' , 'gcs'])
    def test_route_train(self, fixture_processed_1k, model_target):
        """Test route train behave as expected, for various context of LOCAL or GCS model storage"""
    
        # 1) SETUP
        old_model_target = os.environ.get("MODEL_TARGET")
        os.environ.update(MODEL_TARGET=model_target)
    
        data_processed_path = Path(LOCAL_DATA_PATH).joinpath("processed",f"processed_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv")
        data_processed_exists = data_processed_path.is_file()
        if data_processed_exists:
            shutil.copyfile(data_processed_path, f'{data_processed_path}_backup')
            data_processed_path.unlink()
    
        data_processed_fixture_path = "https://storage.googleapis.com/datascience-mlops/taxi-fare-ny/solutions/data_processed_fixture_2009-01-01_2015-01-01_1k.csv"
        os.system(f"curl {data_processed_fixture_path} > {data_processed_path}")
    
        # 2) ACT
        from taxifare.interface.main import train
    
        # Train it from Big Query
>       train(min_date=MIN_DATE, max_date=MAX_DATE, learning_rate=0.01, patience=0)

tests/cloud_training/test_main.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

min_date = '2009-01-01', max_date = '2015-01-01', split_ratio = 0.02
learning_rate = 0.01, batch_size = 256, patience = 0

    def train(
            min_date:str = '2009-01-01',
            max_date:str = '2015-01-01',
            split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set
            learning_rate=0.0005,
            batch_size = 256,
            patience = 2
        ) -> float:
    
        """
        - Download processed data from your BQ table (or from cache if it exists)
        - Train on the preprocessed dataset (which should be ordered by date)
        - Store training results and model weights
    
        Return val_mae as a float
        """
    
        print(Fore.MAGENTA + "\n⭐️ Use case: train" + Style.RESET_ALL)
        print(Fore.BLUE + "\nLoading preprocessed validation data..." + Style.RESET_ALL)
    
        min_date = parse(min_date).strftime('%Y-%m-%d') # e.g '2009-01-01'
        max_date = parse(max_date).strftime('%Y-%m-%d') # e.g '2009-01-01'
    
        # Load processed data using `get_data_with_cache` in chronological order
        # Try it out manually on console.cloud.google.com first!
    
        pass  # YOUR CODE HERE
    
        # Create (X_train_processed, y_train, X_val_processed, y_val)
        pass  # YOUR CODE HERE
    
        # Train model using `model.py`
        pass  # YOUR CODE HERE
    
>       val_mae = np.min(history.history['val_mae'])
E       NameError: name 'history' is not defined

taxifare/interface/main.py:78: NameError
----------------------------- Captured stdout call -----------------------------
[35m
⭐️ Use case: train[0m
[34m
Loading preprocessed validation data...[0m
----------------------------- Captured stderr call -----------------------------
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  153k  100  153k    0     0  1429k      0 --:--:-- --:--:-- --:--:-- 1438k
_________________________ TestMain.test_route_evaluate _________________________

self = <tests.cloud_training.test_main.TestMain object at 0x11992d5a0>

    @patch("taxifare.params.MODEL_TARGET", new='local')
    def test_route_evaluate(self):
        from taxifare.interface.main import evaluate
    
>       mae = evaluate(min_date=MIN_DATE, max_date=MAX_DATE)

tests/cloud_training/test_main.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

min_date = '2009-01-01', max_date = '2015-01-01', stage = 'Production'

    def evaluate(
            min_date:str = '2014-01-01',
            max_date:str = '2015-01-01',
            stage: str = "Production"
        ) -> float:
        """
        Evaluate the performance of the latest production model on processed data
        Return MAE as a float
        """
        print(Fore.MAGENTA + "\n⭐️ Use case: evaluate" + Style.RESET_ALL)
    
        model = load_model(stage=stage)
>       assert model is not None
E       AssertionError

taxifare/interface/main.py:108: AssertionError
----------------------------- Captured stdout call -----------------------------
[35m
⭐️ Use case: evaluate[0m
[34m
Load latest model from local registry...[0m
___________________________ TestMain.test_route_pred ___________________________

self = <tests.cloud_training.test_main.TestMain object at 0x11992e800>

    @patch("taxifare.params.MODEL_TARGET", new='local')
    def test_route_pred(self):
        from taxifare.interface.main import pred
    
>       y_pred = pred()

tests/cloud_training/test_main.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_pred =             pickup_datetime  ...  passenger_count
0 2013-07-06 17:18:00+00:00  ...                1

[1 rows x 6 columns]

    def pred(X_pred: pd.DataFrame = None) -> np.ndarray:
        """
        Make a prediction using the latest trained model
        """
    
        print("\n⭐️ Use case: predict")
    
        if X_pred is None:
            X_pred = pd.DataFrame(dict(
            pickup_datetime=[pd.Timestamp("2013-07-06 17:18:00", tz='UTC')],
            pickup_longitude=[-73.950655],
            pickup_latitude=[40.783282],
            dropoff_longitude=[-73.984365],
            dropoff_latitude=[40.769802],
            passenger_count=[1],
        ))
    
        model = load_model()
>       assert model is not None
E       AssertionError

taxifare/interface/main.py:159: AssertionError
----------------------------- Captured stdout call -----------------------------

⭐️ Use case: predict
[34m
Load latest model from local registry...[0m
________________________________ test_i_am_a_vm ________________________________

    def test_i_am_a_vm():
        """
        Test that this code is being run from a Google VM named as per env variable 'INSTANCE'
        """
    
>       assert platform.node() == INSTANCE, f"You should be running from your instance named '{INSTANCE}'."
E       AssertionError: You should be running from your instance named 'taxi-instance'.
E       assert 'Yulings-MacBook-Pro.local' == 'taxi-instance'
E         
E         - taxi-instance
E         + Yulings-MacBook-Pro.local

tests/cloud_training/test_vm.py:9: AssertionError
=========================== short test summary info ============================
FAILED tests/cloud_training/test_main.py::TestMain::test_route_preprocess - F...
FAILED tests/cloud_training/test_main.py::TestMain::test_route_train[local]
FAILED tests/cloud_training/test_main.py::TestMain::test_route_train[gcs] - N...
FAILED tests/cloud_training/test_main.py::TestMain::test_route_evaluate - Ass...
FAILED tests/cloud_training/test_main.py::TestMain::test_route_pred - Asserti...
FAILED tests/cloud_training/test_vm.py::test_i_am_a_vm - AssertionError: You ...
================== 6 failed, 3 passed, 100 warnings in 13.48s ==================
